<html>
<head><title>RHIPE/Datadr/Trelliscope Cluster Installation</title></head>

<body>
<h1 style="text-align: center;">RHIPE/Datadr/Trelliscope Cluster Installation</h1>
<hr>
<h3>Introduction</h3>
This is a guide to install Rhipe/Datadr/Trelliscope on a multi-node cluster. Installing and running Rhipe requires Hadoop, an embarrassingly parallel computational platform and any flavour of Linux as the operating system. In our document we use Debian Linux derivative - Ubuntu. Version 10.04 or 12.04 of Ubuntu can be used for this installation.  
<br>
<h3>Required Packages</h3>
<p> The following packages (in order of installation) are required to run Rhipe/Datadr/Trelliscope successfully.</p>

<p><b>Note</b>: Some of these packages are updated quite frequently please verify that they are compatible with Rhipe/Datadr/Trelliscope and Hadoop before installing them.</p>
<p>
<ul>
<li> pkg-config (package configuration) </li>
<li> Sun/Oracle Java - 6 or 7 (For 7 use any release before subversion 51) </li>
<li> Protocol Buffer - 2.4.1 </li>
<li> Cloudera Hadoop - cdh3u5 </li>
<li> R - 3.0.1 </li>
<li> Rhipe - 0.73 </li>
<li> Binutils (specific to Ubuntu) </li>
<li> rJava - 0.96 </li>
<li> R packages - codetools, MASS, lattice, boot, shiny, and devtools </li> 
<li> datadr </li>
<li> trelliscope </li>
<li> Rstudio </li>
<li> cmake </li>
<li> Shiny server </li>
</ul>
</p>
<br>
<h3> Installation</h3>
<p> The steps to install the packages must be done on all the machines of the cluster, unless specified. All the packages are installing using command line.</p>
<h3> 1. System Update and Packages</h3>
<p> Firstly, system packages must be updated. </p>
<pre>sudo apt-get update
sudo apt-get -y upgrade </pre>
<p> Now install package config package </p>
<pre>sudo apt-get install pkg-config </pre>
<br>
<h3> 2. Java </h3>
<p> First install the Sun/Oracle Java repository and then install Java package. </p>
<pre>
apt-get install -y python-software-properties
add-apt-repository ppa:webupd8team/java
apt-get update
apt-get install -y oracle-java6-installer
</pre>
<p> Now add the path to the Java package to your environment </p>
<pre>
echo -e "JAVA_HOME=/usr/lib/jvm/java-6-oracle	\
JAVA_BIN=$JAVA_HOME/bin	\
PATH=$PATH:$JAVA_HOME:$JAVA_BIN	\
export JAVA_HOME	\
export JAVA_BIN	\
export PATH" >> /etc/profile
</pre>
<br>
<h3> 3. Protocol Buffers </h3>
<p> Download, compile, and install the protocol buffers. </p>
<pre>
wget http://protobuf.googlecode.com/files/protobuf-2.4.1.tar.gz
sudo tar -xzf protobuf-2.4.1.tar.gz
cd protobuf-2.4.1
sudo ./configure
sudo make
sudo make install
sudo ldconfig
</pre>
<br>
<h3> 4. Hadoop </h3>
<p> Cloudera Hadoop installation is simply done through Aptitude package installer. </p>
<pre>
sudo apt-get install -y hadoop-0.20 hadoop-0.20-conf-pseudo \
hadoop-0.20-datanode hadoop-0.20-doc hadoop-0.20-native \
hadoop-0.20-pipes hadoop-0.20-sbin hadoop-0.20-tasktracker \
hadoop-0.20-namenode hadoop-0.20-secondarynamenode hadoop-0.20-jobtracker
</pre>
</p> Next, add the Hadoop home, binaries, and libraries to environment path. </p>
<pre>
echo -e "export HADOOP=/usr/lib/hadoop-0.20	\
export HADOOP_HOME=$HADOOP	\
export HADOOP_BIN=$HADOOP/bin	\
export HADOOP_LIB=$HADOOP/lib	\
export HADOOP_CONF_DIR=$HADOOP/conf	\
export PATH=$HADOOP:$HADOOP_BIN:$HADOOP_HOME:$HADOOP_LIB:$PATH" >> /etc/bashrc
</pre>
<br>
<h3> 4. R </h3>
<p> 
<pre>
sudo apt-get install -y r-base r-base-dev r-recommended \
r-cran-rodbc ess
</pre>
<br>
<h3> 5. Binutils and rJava (required for Rhipe) </h3>
<p> Binutils and rJava are packages required by Rhipe. Binutils can be installed using the aptitude package installer.</p>
<pre>sudo apt-get install binutils-gold</pre>
<p> Before installing rJava ensure that <code>JAVA_HOME</code> environment variable is pointing to the Sun/Oracle installation of Java. If not, log out and log back into the system. rJava needs to be downloaded and installed using R. </p>
<pre>
wget http://cran.r-project.org/src/contrib/rJava_0.9-6.tar.gz
sudo R CMD INSTALL rJava_0.9-6.tar.gz
</pre>
<br>
<h3> 6. Rhipe </h3>
<p> Rhipe too is to be downloaded and installed using R. </p>
<pre>
wget https://raw.github.com/saptarshiguha/RHIPE/master/code/Rhipe_0.73.tar.gz
R CMD INSTALL Rhipe_0.73.tar.gz
</pre>
<br>
<h3> 7. Packages Required by Trelliscope and Datadr </h3>
<p> R packages - codetools, lattice, MASS, boot, shiny, and devtools are needed for trelliscope and datadr. They are installed by using <code>install.packages</code> command in R. </p>
<pre>sudo R -e "install.packages('codetools', repos='http://cran.rstudio.com/')"
sudo R -e "install.packages('MASS', repos='http://cran.rstudio.com/')"
sudo R -e "install.packages('lattice', repos='http://cran.rstudio.com/')"
sudo R -e "install.packages('boot', repos='http://cran.rstudio.com/')"
sudo R -e "install.packages('shiny', repos='http://cran.rstudio.com/')"
sudo R -e "install.packages('devtools', repos='http://cran.rstudio.com/')
</pre>
<p> openssl package is also required and can be installed with apititude. </p>
<pre>sudo apt-get -y install libcurl4-openssl-dev</pre>
<br>
<h3> 8. Datadr and Trelliscope </h3>
<p> Datadr and trelliscope is installed using <code>install_github</code> function and R. </p>
<pre>
sudo R -e "options(repos = 'http://cran.rstudio.com/'); library(devtools); install_github('datadr', 'hafen')"
sudo R -e "options(repos = 'http://cran.rstudio.com/'); library(devtools); install_github('trelliscope', 'hafen')"
</pre>
<br>
<h3> 9. Rstudio (Optional) </h3>
<p> Rstudio is an optional package. It can be used in lieu of command line R. It needs to be installed only on the Namenode/Jobtracker machine.</p>
<pre>
wget http://download2.rstudio.org/rstudio-server-0.98.507-amd64.deb -O /tmp/rstudio-server.deb
sudo apt-get -y install gdebi-core
sudo gdebi --n /tmp/rstudio-server.deb
</pre>
<br>
<h3> 10. Shiny Server </h3>

<p> Cmake needs to be compiled for shiny server. </p>        
<pre>
wget http://www.cmake.org/files/v2.8/cmake-2.8.12.2.tar.gz
tar xzf cmake-2.8.12.2.tar.gz
cd cmake-2.8.12.2
./configure
make
</pre>
        
<p> Shiny-server code is copied from github and compiled. Configuration file is placed in <code>/etc/shiny-server/</code> </p>
<pre>
git clone https://github.com/rstudio/shiny-server.git
sudo mkdir -p /etc/shiny-server
sudo cp shiny-server/config/default.config /etc/shiny-server/shiny-server.conf

# Get into a temporary directory in which we'll build the project
cd shiny-server
mkdir tmp
cd tmp

# Add the bin directory to the path so we can reference node
DIR=`pwd`
PATH=$PATH:$DIR/../bin/

# See the "Python" section below if your default python version is not 2.6 or 2.7. 
PYTHON=`which python`

# Check the version of Python. If it's not 2.6.x or 2.7.x, see the Python section below.
$PYTHON --version

# Use cmake to prepare the make step. Modify the "--DCMAKE_INSTALL_PREFIX"
# if you wish the install the software at a different location.
../../cmake-2.8.12.2/bin/cmake -DCMAKE_INSTALL_PREFIX=/usr/local -DPYTHON="$PYTHON" ../
# Get an error here? Check the "How do I set the cmake Python version?" question below

# Recompile the npm modules included in the project
make -j4
mkdir ../build
(cd .. && bin/npm --python="$PYTHON" rebuild)

# Need to rebuild our gyp bindings since 'npm rebuild' won't run gyp for us.
(cd .. && ext/node/lib/node_modules/npm/node_modules/node-gyp/bin/node-gyp.js --python="$PYTHON" rebuild)

# Install the software at the predefined location
sudo make install

# POST INSTALL
# Place a shortcut to the shiny-server executable in /usr/bin
sudo ln -s /usr/local/shiny-server/bin/shiny-server /usr/bin/shiny-server
#Create shiny user. On some systems, you may need to specify the full path to 'useradd'
        sudo useradd -r -m shiny

# Create log, config, and application directories
sudo mkdir -p /var/log/shiny-server
sudo mkdir -p /srv/shiny-server
sudo mkdir -p /var/lib/shiny-server
sudo chown shiny /var/log/shiny-server

#copy shiny examples
sudo mkdir /srv/shiny-server/examples
sudo cp -R /usr/local/lib/R/site-library/shiny/examples/* /srv/shiny-server/examples
sudo chown -R shiny:shiny /srv/shiny-server/examples
sudo -u shiny nohup shiny-server &
</pre>
<br>
<h3> Configuration</h3>
<p> Host file and Hadoop configuration files must be modified to reflect the cluster specific parameters. </p>
<h3> 1. Host File</h3>
<p> Host names and host IP addresses of all machines in the Rhipe cluster must be added to host file <code>/etc/hosts</code> on each machine of the cluster. </p> 
<pre>
xxx.xxx.xxx.001 node001
xxx.xxx.xxx.002 node002
...
xxx.xxx.xxx.020 node020
</pre>
<p> <code>xxx.xxx.xxx.001</code> represents the IP address of a machine on the cluster. <code>node001</code> is the hostname of the machine. IP address and hostname can be added/removed to the hosts file as and when machines are added and removed from the cluster. The machine's fully qualified domain name or FQDN can also be used as hostnames.</p>
<br>
<h3> 2. Hadoop <code>core-site.xml</code></h3>
<p>Hadoop core-site.xml file is placed at <code>/etc/hadoop-0.20/conf</code>.</p>
<pre>
&lt;?xml version=&quot;1.0&quot;?&gt;
&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;
&lt;configuration&gt;
&lt;property&gt;
&lt;name&gt;fs.default.name&lt;/name&gt;
&lt;value&gt;hdfs://node001:8020&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
&lt;value&gt;hadoopdir-tmp-1-${username},hadoopdir-tmp-2-${username}&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;fs.checkpoint.dir&lt;/name&gt;
&lt;value&gt;hadoopdir-ck-1,hadoopdir-ck-2&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;fs.trash.interval&lt;/name&gt;
&lt;value&gt;0&lt;/value&gt;
&lt;/property&gt;
</pre>
<p>
<ul>
<li><code>fs.default.name</code> defines your name node. </li>
<li><code>hadoop.tmp.dir</code> defines your temporary storing directory for jobs.</li>
<li><code>fs.checkpoint.dir</code> is where your hdfs checkpoint files are stored.</li>
<li><code>fs.trash.interval</code> is the amount of time deleted files are stored, in seconds.</li>
</ul>
</p>
<br>
<h3> 3. Hadoop <code>mapred-site.xml</code></h3>
<p> Hadoop <code>mapred-site.xml</code> is used to configure job and task trackers. </p>
<pre>
&lt;?xml version=&quot;1.0&quot;?&gt;
&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;
&lt;configuration&gt;
&lt;property&gt;
&lt;name&gt;mapred.job.tracker&lt;/name&gt;
&lt;value&gt;node002/node001:8021&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;mapred.system.dir&lt;/name&gt;
&lt;value&gt;/mapred/system/&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;mapred.local.dir&lt;/name&gt;
&lt;value&gt;/hadoop/data/1/tmp/,/hadoop/data/2/tmp/&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;mapred.job.tracker.handler.count&lt;/name&gt;
&lt;value&gt;n&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;tasktracker.http.threads&lt;/name&gt;
&lt;value&gt;n&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;mapred.tasktracker.map.tasks.maximum&lt;/name&gt;
&lt;value&gt;n/nodes&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;mapred.tasktracker.reduce.tasks.maximum&lt;/name&gt;
&lt;value&gt;n/nodes&lt;/value&gt;
&lt;/property&gt;
</pre>
<p>
<ul>
<li><code>mapred.job.tracker</code> is the hostname of the job tracker node.</li>
<li><code>mapred.system.dir</code> is where map reduce stores its system files.</li>
<li><code>mapred.local.dir</code> is where it stores its working files.</li>
<li><code>mapred.job.tracker.handler.count</code> is the number of job threads.</li>
<li><code>tasktracker.http.threads</code> is equal to the number of job threads.</li>
<li><code>mapred.tasktracker.map.tasks.maximum</code> and <code>mapred.tasktracker.reduce.tasks.maximum</code> are equal
to the number of threads divided by number of nodes.</li>
</ul>
</p>
<br>
<h3> 4. Hadoop <code>hdfs-site.xml</code></h3>
<pre>
&lt;?xml version=&quot;1.0&quot;?&gt;
&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;
&lt;configuration&gt;
&lt;property&gt;
&lt;name&gt;dfs.replication&lt;/name&gt;
&lt;value&gt;n/nodes&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;dfs.data.dir&lt;/name&gt;
&lt;value&gt;/hadoop/data/1/dn,/hadoop/data/2/dn&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;dfs.block.size&lt;/name&gt;
&lt;value&gt;134217728&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;dfs.namenode.handler.count&lt;/name&gt;
&lt;value&gt;n&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;dfs.datanode.max.xcievers&lt;/name&gt;
&lt;value&gt;1024&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
</pre>
<p>
<ul>
<li><code>dfs.replication</code> is the number of times data is replicated.</li>
<li><code>dfs.data.dir</code> is where the data is stored.</li>
<li><code>dfs.block.size</code> is the size of each data block.</li>
<li><code>dfs.namenode.handler.count</code> is the number of threads that the name node will handle. Should
match the number of open threads.</li>
<li><code>dfs.datanode.max.xcievers</code> Maximum number of open files</li>
</ul>
</p>
<br>
<h3> Hadoop Cluster Starting</h3>
<p>The hadoop cluster must be started in the following order</p>
<p>On the Namenode and Jobtracker designated machine start the following. If Namenode and Jobtracker processes are to be run on seperate machines, then run these commands seperately.</p>
<pre>
service hadoop-0.20-namenode start && service hadoop-0.20-jobtracker start
</pre>
<p> On all the machines run the Datanode and Tasktracker processes. If the Datanode and Tasktracker processes are to be run on seperate machines, then start the processes seperately.</p>
<pre>
service hadoop-0.20-datanode start && service hadoop-0.20-tasktracker start
</pre>
<br>

<h3> Notes </h3>
<h3> 1. Node Design </h3>
<p>
<ul>
<li>The Datanode service running on the machines manages the HDFS.</li>
<li>The Namenode service manages all the datanodes and all blocks of the data. It also manages data redundency. For example, when a disk fails Namenode is the service responsible for moving the data
around to other nodes to ensure minimum level of replication.</li>
<li>The Tasktracker service manages the different tasks that are run on each node.</li>
<li>The Jobtracker service manages these Tasktrackers. It keeps information about which tasktracker
is running a job and which of them are not. It also allocates jobs to all the tasktrackers and is
responsible for breaking up the jobs into tasks and distributing it across the cluster.</li>

<li>It must also be noted that in many configurations, the Namenode and the Jobtracker are run on the
same machine. There are a few situations where these two are run separately.</li>
<li>Most times the machine(s) that is/are running Namenode and Jobtracker services also run the Datan-
ode and the Tasktracker service. This is mainly to extract all available computing power.</li>
<li>Under conditions when the machine(s) that is/are running Namenode and Jobtracker services are not
very powerful then the Datanode and the Tasktracker services are not run on them.</li>
<li>The decision to have Datanode and Tasktracker on the Namenode and/or Jobtracker is completely
dependent on the requirements of each environment.</li>
<li>On each machine (Datanode and Tasktracker) majority of the available processor cores are used for
Hadoop and usually a minimal number is kept of other system tasks. For example, on a 4 core Intel
processor, 3 cores are set aside for Hadoop/Map/Reduce, while one core is allocated for system tasks.
Similarly, on a 16 Core AMD processor, anywhere between 12 - 14 cores can be set aside for Hadoop
and the rest for system tasks.</li>
</ul>
</p>

<h3> 2. RAID and Redundancy Design </h3>
<p>
<ul>
<li>RAID configuration are usually not used for HDFS. That is mainly because Hadoop itself handles
redundancy.</li>
<li>But if there is an opportunity to use RAID then it could definitely be used for /boot and / partition.
This is mainly to ensure that when one of the hard drives go down the RAID configured partition on
the other hard drive can seamlessly take over.</li>
<li>The minimum required redundancy configuration for data on Hadoop is 3. This means that each data
block is copied at 2 other locations other than its primary location. Hadoop has shown that with 3
copies of each data block high availability is possible.</li>
</p>
<hr>
</body>
</html>
